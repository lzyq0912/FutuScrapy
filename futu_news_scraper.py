import hashlib
import hmac
import json
import requests
import pandas as pd
import time
import os
import argparse
import schedule
import pytz
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import Dict, Any, List
from tqdm import tqdm
import queue

class ProxyManager:
    """IPä»£ç†æ± ç®¡ç†å™¨"""
    
    def __init__(self):
        self.api_url = "https://find.xiaoxiongip.com/find_http"
        self.api_params = {
            'key': '2034f1ac357f2622',
            'count': '50',  # å¢åŠ åˆ°50ä¸ªIPæé«˜æ± å®¹é‡
            'type': 'text',
            'only': '0',
            'textSep': '3',
            'pw': 'no'
        }
        self.proxy_queue = queue.Queue()
        self.refresh_interval = 150  # 2åˆ†åŠ = 150ç§’
        self.last_refresh = 0
        self.lock = threading.Lock()
        self.is_refreshing = False  # æ·»åŠ åˆ·æ–°çŠ¶æ€æ ‡è®°
        self.failed_refresh_count = 0  # è®°å½•è¿ç»­å¤±è´¥æ¬¡æ•°
        self.max_failed_attempts = 3  # æœ€å¤§å¤±è´¥å°è¯•æ¬¡æ•°
        
        # åˆå§‹åŒ–ä»£ç†æ± 
        self.refresh_proxies()
    
    def get_proxies_from_api(self) -> List[str]:
        """ä»APIè·å–ä»£ç†IPåˆ—è¡¨"""
        try:
            response = requests.get(self.api_url, params=self.api_params, timeout=10)
            if response.status_code == 200:
                # æ–°APIè¿”å›æ ¼å¼: æ¢è¡Œåˆ†å‰²çš„IP:PORT
                proxy_list = [line.strip() for line in response.text.strip().split('\n') if line.strip()]
                print(f"ğŸŒ ä»APIè·å–åˆ° {len(proxy_list)} ä¸ªä»£ç†IP")
                return proxy_list
            elif response.status_code == 502:
                print(f"âŒ ä»£ç†APIæœåŠ¡ä¸å¯ç”¨ (502)ï¼Œå¯èƒ½æ˜¯æœåŠ¡å•†é—®é¢˜")
                return []
            else:
                print(f"âŒ ä»£ç†APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return []
        except requests.exceptions.RequestException as e:
            print(f"âŒ ä»£ç†APIç½‘ç»œå¼‚å¸¸: {e}")
            return []
        except Exception as e:
            print(f"âŒ è·å–ä»£ç†IPå¼‚å¸¸: {e}")
            return []
    
    def refresh_proxies(self):
        """åˆ·æ–°ä»£ç†æ± ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.lock:
            # å¦‚æœå·²ç»åœ¨åˆ·æ–°ä¸­ï¼Œè·³è¿‡
            if self.is_refreshing:
                print("ğŸ”„ ä»£ç†æ± æ­£åœ¨åˆ·æ–°ä¸­ï¼Œè·³è¿‡é‡å¤åˆ·æ–°...")
                return
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¿ç»­å¤±è´¥å¤ªå¤šæ¬¡
            if self.failed_refresh_count >= self.max_failed_attempts:
                print(f"âš ï¸ ä»£ç†APIè¿ç»­å¤±è´¥{self.failed_refresh_count}æ¬¡ï¼Œæš‚æ—¶åœæ­¢åˆ·æ–°")
                return
            
            self.is_refreshing = True
            print("ğŸ”„ åˆ·æ–°ä»£ç†æ± ...")
            
            try:
                proxy_list = self.get_proxies_from_api()
                
                if not proxy_list:
                    self.failed_refresh_count += 1
                    print(f"âŒ ä»£ç†è·å–å¤±è´¥ ({self.failed_refresh_count}/{self.max_failed_attempts})")
                    return
                
                # æˆåŠŸè·å–ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                self.failed_refresh_count = 0
                
                # æ¸…ç©ºæ—§é˜Ÿåˆ—
                while not self.proxy_queue.empty():
                    try:
                        self.proxy_queue.get_nowait()
                    except queue.Empty:
                        break
                
                # æ·»åŠ æ–°ä»£ç†åˆ°é˜Ÿåˆ—
                valid_count = 0
                for proxy in proxy_list:
                    if ':' in proxy and len(proxy.split(':')) == 2:
                        self.proxy_queue.put({
                            'http': f'http://{proxy}',
                            'https': f'http://{proxy}'
                        })
                        valid_count += 1
                
                self.last_refresh = time.time()
                print(f"âœ… ä»£ç†æ± å·²åˆ·æ–°ï¼Œæœ‰æ•ˆä»£ç†: {valid_count} ä¸ª (æ€»è·å–: {len(proxy_list)})")
                
            finally:
                self.is_refreshing = False
    
    def get_proxy(self) -> Dict[str, str]:
        """è·å–ä¸€ä¸ªå¯ç”¨ä»£ç†ï¼ˆä¼˜åŒ–å¤šçº¿ç¨‹å¤„ç†ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å®šæ—¶åˆ·æ–°
        if time.time() - self.last_refresh > self.refresh_interval:
            self.refresh_proxies()
        
        # å°è¯•è·å–ä»£ç†
        try:
            return self.proxy_queue.get_nowait()
        except queue.Empty:
            # æ£€æŸ¥å½“å‰ä»£ç†æ± çŠ¶æ€
            current_size = self.proxy_queue.qsize()
            
            # å¦‚æœä»£ç†æ± ä¸ºç©ºä¸”æ²¡æœ‰åœ¨åˆ·æ–°ï¼Œå°è¯•åˆ·æ–°
            if current_size == 0 and not self.is_refreshing:
                print("âš ï¸ ä»£ç†æ± ä¸ºç©ºï¼Œå°è¯•åˆ·æ–°...")
                self.refresh_proxies()
                
                # å†æ¬¡å°è¯•è·å–
                try:
                    return self.proxy_queue.get_nowait()
                except queue.Empty:
                    print("âŒ åˆ·æ–°åä»æ— å¯ç”¨ä»£ç†")
                    return None
            elif self.is_refreshing:
                print("â³ ä»£ç†æ± åˆ·æ–°ä¸­ï¼Œç­‰å¾…...")
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´åé‡è¯•
                time.sleep(0.1)
                try:
                    return self.proxy_queue.get_nowait()
                except queue.Empty:
                    print("âŒ ç­‰å¾…åä»æ— å¯ç”¨ä»£ç†")
                    return None
            else:
                print("âŒ æ— å¯ç”¨ä»£ç†")
                return None
    
    def return_proxy(self, proxy: Dict[str, str], is_valid: bool = True):
        """å½’è¿˜ä»£ç†åˆ°æ± ä¸­ï¼ˆåªå½’è¿˜æœ‰æ•ˆçš„ä»£ç†ï¼‰"""
        if proxy and is_valid:
            # æ£€æŸ¥ä»£ç†æ ¼å¼æ˜¯å¦æ­£ç¡®
            if 'http' in proxy and 'https' in proxy:
                self.proxy_queue.put(proxy)
            else:
                print(f"âš ï¸ ä»£ç†æ ¼å¼æ— æ•ˆï¼Œä¸å½’è¿˜: {proxy}")
        elif proxy and not is_valid:
            print(f"ğŸš« ä»£ç†æ— æ•ˆï¼Œä¸å½’è¿˜åˆ°æ± ä¸­: {proxy.get('http', 'Unknown')}")
    
    def get_pool_status(self):
        """è·å–ä»£ç†æ± çŠ¶æ€ä¿¡æ¯"""
        return {
            'current_size': self.proxy_queue.qsize(),
            'is_refreshing': self.is_refreshing,
            'last_refresh': self.last_refresh,
            'time_since_refresh': time.time() - self.last_refresh
        }

class FutuTokenGenerator:
    """å¯Œé€”ç‰›ç‰›tokenç”Ÿæˆå™¨ï¼ŒåŸºäºJSé€†å‘åˆ†æ"""
    
    def hmac_sha512(self, text: str, key: str) -> str:
        return hmac.new(key.encode(), text.encode(), hashlib.sha512).hexdigest()
    
    def sha256(self, text: str) -> str:
        return hashlib.sha256(text.encode()).hexdigest()
    
    def serialize_params(self, params: Dict[str, Any]) -> str:
        filtered_params = {}
        for key, value in params.items():
            if value is not None:
                filtered_params[key] = str(value)
        return json.dumps(filtered_params, separators=(',', ':'))
    
    def generate_quote_token(self, params: Dict[str, Any]) -> str:
        serialized = self.serialize_params(params) if params else "{}"
        if len(serialized) <= 0:
            serialized = "quote"
        hmac_result = self.hmac_sha512(serialized, "quote_web")
        hmac_slice = hmac_result[:10]
        sha_result = self.sha256(hmac_slice)
        return sha_result[:10]

class FutuNewsScraper:
    """å¯Œé€”ç‰›ç‰›æ–°é—»çˆ¬è™« - æ‰¹é‡è·å–æ¸¯ç¾è‚¡æ–°é—»ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, max_workers: int = 30, max_concurrent: int = 25, request_delay: float = 0.1, 
                 output_dir: str = None, use_proxy: bool = True):
        self.token_generator = FutuTokenGenerator()
        self.base_url = "https://www.futunn.com"
        
        # ä»£ç†ç®¡ç†
        self.use_proxy = use_proxy
        self.proxy_downgraded = False  # æ ‡è®°æ˜¯å¦å·²é™çº§
        print(f"ğŸ” ä»£ç†é…ç½®è°ƒè¯•: use_proxy={use_proxy}")
        if use_proxy:
            print("ğŸŒ åˆå§‹åŒ–IPä»£ç†æ± ...")
            try:
                self.proxy_manager = ProxyManager()
                # æ˜¾ç¤ºåˆå§‹ä»£ç†æ± çŠ¶æ€
                status = self.proxy_manager.get_pool_status()
                print(f"âœ… ä»£ç†æ± åˆå§‹åŒ–æˆåŠŸï¼Œå½“å‰çŠ¶æ€: {status['current_size']}ä¸ªä»£ç†")
                
                # æ£€æŸ¥ä»£ç†æ± æ˜¯å¦ä¸ºç©ºï¼ˆAPIå¤±è´¥çš„æƒ…å†µï¼‰
                if status['current_size'] == 0 and self.proxy_manager.failed_refresh_count >= 1:
                    print("âš ï¸ ä»£ç†æ± åˆå§‹åŒ–åä¸ºç©ºï¼Œå¯èƒ½æ˜¯APIé—®é¢˜ï¼Œå»ºè®®é™çº§åˆ°æ— ä»£ç†æ¨¡å¼")
                    
            except Exception as e:
                print(f"âŒ ä»£ç†æ± åˆå§‹åŒ–å¤±è´¥: {e}")
                self.proxy_manager = None
        else:
            print("ğŸš« ä»£ç†æ± å·²ç¦ç”¨")
            self.proxy_manager = None
        
        # å¹¶å‘æ§åˆ¶å‚æ•°ï¼ˆæå‡æ€§èƒ½ï¼‰
        self.max_workers = max_workers
        self.max_concurrent = max_concurrent
        self.request_delay = request_delay
        self.semaphore = threading.Semaphore(max_concurrent)
        
        # åŸºç¡€headers
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Origin': 'https://www.futunn.com',
            'Referer': 'https://www.futunn.com/quote/us',
        }
        
        # è¾“å‡ºç›®å½•é…ç½®
        self.output_dir = output_dir or 'output'  # é»˜è®¤æœ¬åœ°outputç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # å¸‚åœºç±»å‹æ˜ å°„
        self.market_type_map = {
            'HK': 1,  # æ¸¯è‚¡
            'US': 2,  # ç¾è‚¡
        }
        
        # è¿›åº¦è·Ÿè¸ª
        self.progress_lock = threading.Lock()
        self.processed_stocks = 0
        self.total_news = 0
        self.valid_news = 0
        self.skipped_stocks = 0
        self.error_count = 0
        self.start_time = None
        self.progress_bar = None
    
    def load_stock_data(self) -> pd.DataFrame:
        """åŠ è½½è‚¡ç¥¨æ•°æ®ï¼Œç­›é€‰æ¸¯è‚¡å’Œç¾è‚¡"""
        try:
            df = pd.read_csv('all_stocks_info.csv')
            print(f"ğŸ“Š åŠ è½½è‚¡ç¥¨æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # ç­›é€‰æ¸¯è‚¡å’Œç¾è‚¡
            hk_us_stocks = df[df['code'].str.startswith(('HK.', 'US.'))]
            print(f"ğŸ¯ æ¸¯è‚¡å’Œç¾è‚¡æ•°é‡: {len(hk_us_stocks)} æ¡")
            
            # æŒ‰å¸‚åœºåˆ†ç»„ç»Ÿè®¡
            market_counts = hk_us_stocks['code'].str[:2].value_counts()
            for market, count in market_counts.items():
                print(f"  - {market}è‚¡: {count} åª")
            
            return hk_us_stocks
            
        except Exception as e:
            print(f"âŒ åŠ è½½è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def is_news_within_days(self, news_timestamp: int, days_limit: int = 0) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦åœ¨æŒ‡å®šå¤©æ•°å†…ï¼Œdays_limit=0è¡¨ç¤ºä»Šå¤©"""
        try:
            news_date = datetime.fromtimestamp(news_timestamp).date()
            current_date = datetime.now().date()
            days_diff = (current_date - news_date).days
            return days_diff <= days_limit
        except:
            return False
    
    def format_date(self, dt: datetime = None, format_type: str = 'datetime') -> str:
        """ç»Ÿä¸€æ—¥æœŸæ ¼å¼åŒ–å·¥å…·"""
        if dt is None:
            dt = datetime.now()
        
        formats = {
            'date': '%Y%m%d',           # 20250730
            'datetime': '%Y-%m-%d %H:%M:%S',  # 2025-07-30 21:30:00
            'date_dash': '%Y-%m-%d',    # 2025-07-30
            'timestamp': '%H%M%S'       # 213000
        }
        return dt.strftime(formats.get(format_type, formats['datetime']))
    
    def update_progress(self, valid_news_count: int = 0, total_news_count: int = 0, 
                       skipped: bool = False, error: bool = False):
        """æ›´æ–°è¿›åº¦ä¿¡æ¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.progress_lock:
            self.processed_stocks += 1
            self.valid_news += valid_news_count
            self.total_news += total_news_count
            if skipped:
                self.skipped_stocks += 1
            if error:
                self.error_count += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            if self.progress_bar:
                self.progress_bar.update(1)
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                desc = f"æ–°é—»:{self.valid_news} è·³è¿‡:{self.skipped_stocks} é”™è¯¯:{self.error_count}"
                self.progress_bar.set_description(desc)
    
    def get_stock_news(self, stock_info: Dict[str, str], max_news_per_stock: int = 15, 
                      retry_times: int = 2) -> List[Dict[str, Any]]:
        """è·å–å•åªè‚¡ç¥¨çš„æ–°é—»åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        stock_id = stock_info['stock_id']
        stock_code = stock_info['code']
        market_prefix = stock_code[:2]
        market_type = self.market_type_map.get(market_prefix, 2)
        
        # åˆ›å»ºä¼šè¯
        session = requests.Session()
        session.headers.update(self.headers)
        all_news = []
        
        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        with self.semaphore:
            for attempt in range(retry_times + 1):
                try:
                    current_seq_mark = None
                    
                    # æœ€å¤šè·å–5é¡µæ•°æ®ï¼Œç¡®ä¿å……åˆ†è·å–
                    for page in range(5):
                        params = {
                            'stock_id': stock_id,
                            'market_type': market_type,
                            'type': 0,
                            'subType': 0,
                        }
                        
                        if current_seq_mark:
                            params['seq_mark'] = current_seq_mark
                        
                        # ç”Ÿæˆquote-token
                        quote_token = self.token_generator.generate_quote_token(params)
                        url = f"{self.base_url}/quote-api/quote-v2/get-news-list"
                        
                        request_headers = self.headers.copy()
                        request_headers['quote-token'] = quote_token
                        
                        # è·å–ä»£ç†å¹¶å‘é€è¯·æ±‚
                        proxy = None
                        proxy_is_valid = True
                        
                        # æ£€æŸ¥ä»£ç†æ± çŠ¶æ€ï¼Œå¦‚æœæŒç»­æ— æ•ˆåˆ™é™çº§
                        if (self.use_proxy and self.proxy_manager and 
                            not self.proxy_downgraded and 
                            self.proxy_manager.failed_refresh_count >= self.proxy_manager.max_failed_attempts):
                            print("ğŸ”» ä»£ç†æ± æŒç»­å¤±æ•ˆï¼Œè‡ªåŠ¨é™çº§ä¸ºæ— ä»£ç†æ¨¡å¼")
                            self.proxy_downgraded = True
                        
                        if (self.use_proxy and self.proxy_manager and not self.proxy_downgraded):
                            proxy = self.proxy_manager.get_proxy()
                            if not proxy:
                                print("âš ï¸ æš‚æ— å¯ç”¨ä»£ç†ï¼Œæœ¬æ¬¡è¯·æ±‚ä½¿ç”¨ç›´è¿")
                        
                        try:
                            response = session.get(url, params=params, headers=request_headers, 
                                                 proxies=proxy, timeout=15)
                        except (requests.exceptions.ProxyError, 
                               requests.exceptions.ConnectTimeout,
                               requests.exceptions.ConnectionError) as e:
                            # ä»£ç†ç›¸å…³é”™è¯¯ï¼Œæ ‡è®°ä»£ç†æ— æ•ˆ
                            proxy_is_valid = False
                            raise
                        finally:
                            # å½’è¿˜ä»£ç†ï¼ˆåªå½’è¿˜æœ‰æ•ˆçš„ä»£ç†ï¼‰
                            if proxy and self.proxy_manager:
                                self.proxy_manager.return_proxy(proxy, proxy_is_valid)
                        
                        if response.status_code == 200:
                            data = response.json()
                            if data.get('code') == 0 and data.get('data', {}).get('list'):
                                news_list = data['data']['list']
                                all_news.extend(news_list)
                                
                                if len(all_news) >= max_news_per_stock:
                                    all_news = all_news[:max_news_per_stock]
                                    break
                                
                                next_seq_mark = data['data'].get('seq_mark')
                                if not next_seq_mark or next_seq_mark == current_seq_mark:
                                    break
                                
                                current_seq_mark = next_seq_mark
                            else:
                                break
                        elif response.status_code == 429:
                            # IPè¢«é™æµï¼Œä¸å½’è¿˜è¿™ä¸ªä»£ç†ï¼Œç›´æ¥è·å–æ–°ä»£ç†é‡è¯•
                            if attempt < retry_times:
                                time.sleep(0.2)  # çŸ­æš‚å»¶è¿Ÿ
                                continu 
                    
                            else:
                                break
                        elif response.status_code == 403:
                            # IPè¢«ç¦ï¼Œåˆ‡æ¢ä»£ç†é‡è¯•
                            if attempt < retry_times:
                                time.sleep(0.2)
                                continue
                            else:
                                break
                        else:
                            if attempt < retry_times:
                                time.sleep(0.5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿé‡è¯•
                                continue
                            else:
                                break
                        
                        # è¯·æ±‚é—´éš”
                        time.sleep(self.request_delay)
                    
                    # è§£ææ–°é—»æ•°æ®ï¼ˆåªä¿ç•™ä»Šæ—¥æ–°é—»ï¼‰
                    if all_news:
                        parsed_news = self.parse_news_data(all_news, stock_info, only_today=True)
                        self.update_progress(valid_news_count=len(parsed_news), total_news_count=len(all_news))
                        return parsed_news
                    else:
                        self.update_progress(skipped=True)
                        return []
                        
                except requests.exceptions.RequestException:
                    if attempt < retry_times:
                        print(f"    ğŸ”„ é‡è¯• {stock_code} (å°è¯• {attempt+2}/{retry_times+1}): ç½‘ç»œè¯·æ±‚å¼‚å¸¸")
                        time.sleep(1 * (attempt + 1))
                        continue
                    else:
                        print(f"    âŒ {stock_code} æœ€ç»ˆå¤±è´¥: ç½‘ç»œè¯·æ±‚å¼‚å¸¸")
                        self.update_progress(error=True)
                        return []
                except Exception:
                    if attempt < retry_times:
                        print(f"    ğŸ”„ é‡è¯• {stock_code} (å°è¯• {attempt+2}/{retry_times+1}): æœªçŸ¥é”™è¯¯")
                        time.sleep(1 * (attempt + 1))
                        continue
                    else:
                        print(f"    âŒ {stock_code} æœ€ç»ˆå¤±è´¥: æœªçŸ¥é”™è¯¯")
                        self.update_progress(error=True)
                        return []
        
        print(f"    âŒ {stock_code} æ‰€æœ‰é‡è¯•å¤±è´¥")
        self.update_progress(error=True)
        return []
    
    def parse_news_data(self, news_list: List[Dict[str, Any]], 
                       stock_info: Dict[str, str], only_today: bool = True) -> List[Dict[str, Any]]:
        """è§£ææ–°é—»æ•°æ®ä¸ºç»“æ„åŒ–æ ¼å¼ï¼Œå¯é€‰æ‹©åªä¿ç•™ä»Šæ—¥æ–°é—»"""
        parsed_news = []
        
        for news_item in news_list:
            try:
                # æ£€æŸ¥å¿…è¦å­—æ®µ
                news_timestamp = news_item.get('time', 0)
                if not news_timestamp:
                    continue  # è·³è¿‡æ— æ—¶é—´æˆ³çš„æ–°é—»
                
                # å¦‚æœå¼€å¯ä»Šæ—¥æ–°é—»è¿‡æ»¤ï¼Œåªä¿ç•™ä»Šå¤©çš„æ–°é—»
                if only_today and not self.is_news_within_days(news_timestamp, 0):
                    continue
                
                # åŸºç¡€è‚¡ç¥¨ä¿¡æ¯
                record = {
                    'è‚¡ç¥¨ä»£ç ': stock_info.get('code', ''),
                    'å…¬å¸åç§°': stock_info.get('stock_name', ''),
                    'è‚¡ç¥¨ID': stock_info.get('stock_id', ''),
                    'å¸‚åœº': stock_info.get('code', '')[:2],  # HK æˆ– US
                }
                
                # æ–°é—»ä¿¡æ¯ - ä½¿ç”¨APIå®é™…è¿”å›çš„å­—æ®µå
                record.update({
                    'æ–°é—»ID': news_item.get('id', ''),
                    'æ–°é—»æ ‡é¢˜': news_item.get('title', ''),
                    'å‘å¸ƒæ—¶é—´': self.format_date(
                        datetime.fromtimestamp(int(news_timestamp))
                    ),
                    'æ–°é—»æ¥æº': news_item.get('source', ''),
                    'æ–°é—»æ‘˜è¦': news_item.get('abstract', ''),  # å¤§éƒ¨åˆ†ä¸ºç©ºï¼Œä½†ä¿ç•™å­—æ®µ
                    'æ–°é—»é“¾æ¥': news_item.get('url', ''),
                    'é‡è¦æ€§çº§åˆ«': news_item.get('impt_lvl', 0),
                    'é‡è¦æ€§æ ‡ç­¾': news_item.get('impt_tag', ''),
                    'é“¾æ¥ç±»å‹': news_item.get('link_type', 0),
                })
                
                # å¤„ç†å‘å¸ƒæ—¥æœŸ
                publish_date = datetime.fromtimestamp(int(news_timestamp))
                record['å‘å¸ƒæ—¥æœŸ'] = self.format_date(publish_date, format_type='date_dash')
                record['å‘å¸ƒå°æ—¶'] = publish_date.hour
                
                # æ·»åŠ æ—¶æ•ˆæ€§æ ‡è®°ï¼ˆç”¨äºåç»­åˆ†æï¼Œä½†ä¸è¿‡æ»¤ï¼‰
                days_diff = (datetime.now() - publish_date).days
                record['å¤©æ•°å·®'] = days_diff
                record['æ˜¯å¦3å¤©å†…'] = days_diff <= 3
                
                parsed_news.append(record)
                
            except Exception:
                # è®°å½•è§£æé”™è¯¯ï¼Œä½†ç»§ç»­å¤„ç†å…¶ä»–æ–°é—»
                print(f"    âš ï¸ è§£ææ–°é—»å¤±è´¥")
                continue
        
        return parsed_news
    
    def batch_scrape_news(self, target_date: str = None, 
                         max_stocks: int = 100, 
                         max_news_per_stock: int = 15,
                         market_filter: str = 'all') -> List[Dict[str, Any]]:
        """æ‰¹é‡çˆ¬å–æ–°é—»ï¼ˆå¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆï¼‰"""
        
        if not target_date:
            target_date = self.format_date(format_type='date')
        
        print(f"ğŸš€ å¼€å§‹å¤šçº¿ç¨‹çˆ¬å–æ–°é—» ({self.max_workers}çº¿ç¨‹, {self.max_concurrent}å¹¶å‘)")
        print(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date} | ğŸ¯ æœ€å¤§è‚¡ç¥¨æ•°: {max_stocks}")
        print(f"ğŸ“° æ¯è‚¡ç¥¨æœ€å¤§æ–°é—»æ•°: {max_news_per_stock} | ğŸ¢ å¸‚åœºè¿‡æ»¤: {market_filter}")
        
        # åŠ è½½è‚¡ç¥¨æ•°æ®
        stock_df = self.load_stock_data()
        if stock_df.empty:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°è‚¡ç¥¨æ•°æ®")
            return []
        
        # æŒ‰å¸‚åœºè¿‡æ»¤
        if market_filter != 'all':
            stock_df = stock_df[stock_df['code'].str.startswith(f'{market_filter}.')]
        
        # é™åˆ¶è‚¡ç¥¨æ•°é‡
        if max_stocks < len(stock_df):
            stock_df = stock_df.head(max_stocks)
        
        # å‡†å¤‡è‚¡ç¥¨ä¿¡æ¯åˆ—è¡¨
        stock_list = []
        for _, row in stock_df.iterrows():
            stock_list.append({
                'stock_id': str(row['stock_id']),
                'code': row['code'],
                'stock_name': row['stock_name'],
            })
        
        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
        self.processed_stocks = 0
        self.valid_news = 0
        self.total_news = 0
        self.skipped_stocks = 0
        self.error_count = 0
        self.start_time = time.time()
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        with self.progress_lock:
            self.progress_bar = tqdm(
                total=len(stock_list), 
                desc="æ–°é—»:0 è·³è¿‡:0 é”™è¯¯:0",
                unit="è‚¡ç¥¨",
                ncols=100,
                position=0,
                leave=True
            )
        
        all_news = []
        
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_stock = {
                    executor.submit(self.get_stock_news, stock_info, max_news_per_stock): stock_info 
                    for stock_info in stock_list
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_stock):
                    try:
                        news_list = future.result()
                        if news_list:
                            all_news.extend(news_list)
                    except Exception:
                        # é”™è¯¯å·²åœ¨get_stock_newsä¸­å¤„ç†
                        pass
        finally:
            # å…³é—­è¿›åº¦æ¡
            self.progress_bar.close()
        
        # å®Œæˆç»Ÿè®¡
        elapsed_time = time.time() - self.start_time
        effective_stocks = len(stock_list) - self.skipped_stocks - self.error_count
        
        print(f"\nğŸ“Š çˆ¬å–å®Œæˆ")
        print(f"â° æ€»è€—æ—¶: {int(elapsed_time//60)}åˆ†{int(elapsed_time%60)}ç§’")
        print(f"âœ… æœ‰æ•ˆè‚¡ç¥¨: {effective_stocks}/{len(stock_list)}")
        print(f"ğŸ“­ è·³è¿‡è‚¡ç¥¨: {self.skipped_stocks}åª (æ— æ–°é—»æ•°æ®)")
        print(f"âŒ å¤±è´¥è‚¡ç¥¨: {self.error_count}åª (ç½‘ç»œæˆ–å…¶ä»–é”™è¯¯)")
        print(f"ğŸ“° æ€»æ–°é—»æ•°: {len(all_news)}æ¡ (åŸå§‹è·å–: {self.total_news}æ¡)")
        
        # ç»Ÿè®¡æ—¶æ•ˆæ€§åˆ†å¸ƒ
        if all_news:
            recent_news = len([n for n in all_news if n.get('æ˜¯å¦3å¤©å†…', False)])
            print(f"ğŸ• æ—¶æ•ˆåˆ†å¸ƒ: 3å¤©å†…{recent_news}æ¡ ({recent_news/len(all_news)*100:.1f}%), è¶…è¿‡3å¤©{len(all_news)-recent_news}æ¡")
        
        if elapsed_time > 0:
            print(f"ğŸ“ˆ å¹³å‡æ•ˆç‡: {len(all_news)/elapsed_time:.1f}æ¡/ç§’")
        
        # ä¿å­˜åˆ°CSV
        if all_news:
            self.save_news_to_csv(all_news, target_date, market_filter)
        
        return all_news
    
    def save_news_to_csv(self, news_data: List[Dict[str, Any]], 
                        date_str: str, market_filter: str = 'all'):
        """ä¿å­˜æ–°é—»æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not news_data:
            print("âš ï¸ æ²¡æœ‰æ–°é—»æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        df = pd.DataFrame(news_data)
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        df = df.sort_values('å‘å¸ƒæ—¶é—´', ascending=False)
        
        # 1. ä¿å­˜æ‰€æœ‰æ–°é—»ï¼ˆ3å¤©å†…çš„ï¼‰
        recent_df = df[df['æ˜¯å¦3å¤©å†…'] == True].copy()
        if len(recent_df) > 0:
            # æ„å»ºæ–‡ä»¶å
            timestamp = self.format_date(format_type='timestamp')
            if market_filter == 'all':
                filename = f"news_3days_{date_str}_{timestamp}.csv"
            else:
                filename = f"news_{market_filter}_3days_{date_str}_{timestamp}.csv"
            
            filepath = os.path.join(self.output_dir, filename)
            
            # ä¿å­˜ä¸ºCSVï¼ŒåŒ…å«BOMä»¥æ”¯æŒä¸­æ–‡
            recent_df.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"âœ… 3å¤©å†…æ–°é—»å·²ä¿å­˜: {filename}")
            print(f"ğŸ“Š å…±ä¿å­˜ {len(recent_df)} æ¡æ–°é—»è®°å½•")
        
        # 2. å•ç‹¬ä¿å­˜å½“å¤©æ–°é—»
        today_str = self.format_date(format_type='date_dash')
        today_df = df[df['å‘å¸ƒæ—¥æœŸ'] == today_str].copy()
        if len(today_df) > 0:
            timestamp = self.format_date(format_type='timestamp')
            if market_filter == 'all':
                today_filename = f"news_today_{date_str}_{timestamp}.csv"
            else:
                today_filename = f"news_{market_filter}_today_{date_str}_{timestamp}.csv"
            
            today_filepath = os.path.join(self.output_dir, today_filename)
            today_df.to_csv(today_filepath, index=False, encoding='utf-8-sig')
            print(f"âœ… å½“å¤©æ–°é—»å·²ä¿å­˜: {today_filename}")
            print(f"ğŸ“Š å½“å¤©æ–°é—» {len(today_df)} æ¡")
        
        # æ˜¾ç¤ºæ•°æ®æ±‡æ€»ï¼ˆåŸºäº3å¤©å†…æ–°é—»ï¼‰
        if len(recent_df) > 0:
            print(f"\nğŸ“ˆ æ•°æ®æ±‡æ€» (3å¤©å†…æ–°é—»):")
            print(f"æ€»æ–°é—»: {len(recent_df)} æ¡ | æ¶‰åŠè‚¡ç¥¨: {recent_df['è‚¡ç¥¨ä»£ç '].nunique()} åª | å¸‚åœº: {', '.join(recent_df['å¸‚åœº'].unique())}")
            
            if 'æ–°é—»æ¥æº' in recent_df.columns:
                top_sources = recent_df['æ–°é—»æ¥æº'].value_counts().head(3)
                sources_str = ' | '.join([f"{source}({count})" for source, count in top_sources.items()])
                print(f"ä¸»è¦æ¥æº: {sources_str}")
            
            has_summary = len(recent_df[recent_df['æ–°é—»æ‘˜è¦'].str.len() > 0])
            print(f"æœ‰æ‘˜è¦: {has_summary}/{len(recent_df)} ({has_summary/len(recent_df)*100:.1f}%)")
            
            return filepath if len(recent_df) > 0 else None
        
        return None
    
    def cleanup_old_news(self, keep_days: int = 3):
        """æ¸…ç†æ—§æ–°é—»æ–‡ä»¶ï¼Œä¿æŒæŒ‡å®šå¤©æ•°çš„æ•°æ®"""
        try:
            output_dir = self.output_dir
            if not os.path.exists(output_dir):
                return
            
            cutoff_date = datetime.now() - timedelta(days=keep_days)
            cutoff_str = self.format_date(cutoff_date, format_type='date')
            
            removed_count = 0
            for filename in os.listdir(output_dir):
                if filename.startswith('news_') and filename.endswith('.csv'):
                    # æå–æ–‡ä»¶ä¸­çš„æ—¥æœŸ
                    parts = filename.split('_')
                    if len(parts) >= 2:
                        try:
                            file_date = parts[1]  # news_20250720_xxxx.csv
                            if len(file_date) == 8 and file_date < cutoff_str:
                                file_path = os.path.join(output_dir, filename)
                                os.remove(file_path)
                                removed_count += 1
                                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ–‡ä»¶: {filename}")
                        except:
                            continue
            
            if removed_count > 0:
                print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {removed_count} ä¸ªæ—§æ–‡ä»¶")
            else:
                print("ğŸ“ æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ–‡ä»¶")
                
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—§æ–‡ä»¶å¤±è´¥: {e}")
    
    def check_existing_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†å²æ•°æ®"""
        if not os.path.exists(self.output_dir):
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰3å¤©å†…çš„æ–°é—»æ–‡ä»¶
        try:
            cutoff_date = datetime.now() - timedelta(days=3)
            cutoff_str = self.format_date(cutoff_date, format_type='date')
            
            for filename in os.listdir(self.output_dir):
                if filename.startswith('news_') and filename.endswith('.csv'):
                    parts = filename.split('_')
                    if len(parts) >= 2 and len(parts[1]) == 8 and parts[1] >= cutoff_str:
                        print(f"ğŸ“Š å‘ç°è¿‘æœŸæ•°æ®æ–‡ä»¶")
                        return True
            return False
        except Exception:
            return False
    
    def run_auto_mode(self):
        """è‡ªåŠ¨æ¨¡å¼ï¼šæ™ºèƒ½æ•°æ®ç®¡ç† + å®šæ—¶è¿è¡Œ"""
        print("ğŸ¤– å¯åŠ¨è‡ªåŠ¨æ¨¡å¼")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†å²æ•°æ®
        has_existing_data = self.check_existing_data()
        
        if not has_existing_data:
            print("ğŸ†• é¦–æ¬¡è¿è¡Œï¼Œçˆ¬å–3å¤©å†å²æ•°æ®")
            # é¦–æ¬¡è¿è¡Œï¼Œçˆ¬å–3å¤©æ•°æ®
            for i in range(3):
                target_date = self.format_date(datetime.now() - timedelta(days=i), format_type='date')
                print(f"\nğŸ“… çˆ¬å–æ—¥æœŸ: {target_date}")
                
                # åˆ†åˆ«çˆ¬å–æ¸¯è‚¡å’Œç¾è‚¡
                self.batch_scrape_news(
                    target_date=target_date,
                    max_stocks=200,
                    max_news_per_stock=15,
                    market_filter='HK'
                )
                
                self.batch_scrape_news(
                    target_date=target_date,
                    max_stocks=200,
                    max_news_per_stock=15,
                    market_filter='US'
                )
                
                time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
        else:
            print("ğŸ“ˆ å¢é‡æ›´æ–°æ¨¡å¼ï¼Œçˆ¬å–ä»Šæ—¥æ•°æ®")
            # å¢é‡æ›´æ–°ï¼Œåªçˆ¬å–ä»Šå¤©çš„æ•°æ®
            today = self.format_date(format_type='date')
            
            # åˆ†åˆ«çˆ¬å–æ¸¯è‚¡å’Œç¾è‚¡
            self.batch_scrape_news(
                target_date=today,
                max_stocks=200,
                max_news_per_stock=15,
                market_filter='HK'
            )
            
            self.batch_scrape_news(
                target_date=today,
                max_stocks=200,
                max_news_per_stock=15,
                market_filter='US'
            )
        
        # æ¸…ç†3å¤©å‰çš„æ—§æ•°æ®
        print("\nğŸ§¹ æ¸…ç†æ—§æ•°æ®")
        self.cleanup_old_news(keep_days=3)
        
        print("âœ… è‡ªåŠ¨æ¨¡å¼è¿è¡Œå®Œæˆ")
    
    def run_test_mode(self):
        """æµ‹è¯•æ¨¡å¼ï¼šçˆ¬å–100åªè‚¡ç¥¨éªŒè¯åŠŸèƒ½"""
        print("ğŸ§ª å¯åŠ¨æµ‹è¯•æ¨¡å¼")
        
        today = self.format_date(format_type='date')
        
        # æµ‹è¯•çˆ¬å–ï¼š50åªæ¸¯è‚¡ + 50åªç¾è‚¡
        print("\n--- æµ‹è¯•æ¸¯è‚¡ ---")
        self.batch_scrape_news(
            target_date=today,
            max_stocks=50,
            max_news_per_stock=10,
            market_filter='HK'
        )
        
        print("\n--- æµ‹è¯•ç¾è‚¡ ---")
        self.batch_scrape_news(
            target_date=today,
            max_stocks=50,
            max_news_per_stock=10,
            market_filter='US'
        )
        
        print("âœ… æµ‹è¯•æ¨¡å¼è¿è¡Œå®Œæˆ")
    
    def run_full_test_mode(self):
        """å…¨é‡æµ‹è¯•æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰æ¸¯ç¾è‚¡3å¤©æ–°é—»"""
        print("ğŸš€ å¯åŠ¨å…¨é‡æµ‹è¯•æ¨¡å¼")
        
        today = self.format_date(format_type='date')
        
        # å…¨é‡çˆ¬å–ï¼šæ‰€æœ‰æ¸¯è‚¡
        print("\n--- å…¨é‡çˆ¬å–æ¸¯è‚¡ ---")
        self.batch_scrape_news(
            target_date=today,
            max_stocks=99999,  # ä¸é™åˆ¶æ•°é‡
            max_news_per_stock=15,  # æ¯åªè‚¡ç¥¨æœ€å¤š15æ¡æ–°é—»
            market_filter='HK'
        )
        
        # å…¨é‡çˆ¬å–ï¼šæ‰€æœ‰ç¾è‚¡
        print("\n--- å…¨é‡çˆ¬å–ç¾è‚¡ ---")
        self.batch_scrape_news(
            target_date=today,
            max_stocks=99999,  # ä¸é™åˆ¶æ•°é‡
            max_news_per_stock=15,  # æ¯åªè‚¡ç¥¨æœ€å¤š15æ¡æ–°é—»
            market_filter='US'
        )
        
        print("âœ… å…¨é‡æµ‹è¯•æ¨¡å¼è¿è¡Œå®Œæˆ")
    
    def schedule_auto_mode(self):
        """è°ƒåº¦è‡ªåŠ¨æ¨¡å¼ï¼Œæ¯å¤©æ™šä¸Š8ç‚¹åŒ—äº¬æ—¶é—´è¿è¡Œ"""
        # è®¾ç½®åŒ—äº¬æ—¶åŒº
        beijing_tz = pytz.timezone('Asia/Shanghai')
        
        def job():
            print(f"\nâ° {datetime.now(beijing_tz).strftime('%Y-%m-%d %H:%M:%S')} åŒ—äº¬æ—¶é—´ - å¼€å§‹å®šæ—¶ä»»åŠ¡")
            self.run_auto_mode()
        
        # æ¯å¤©æ™šä¸Š8ç‚¹è¿è¡Œ
        schedule.every().day.at("20:00").do(job)
        
        print("ğŸ“… å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼šæ¯å¤©æ™šä¸Š8ç‚¹åŒ—äº¬æ—¶é—´è¿è¡Œ")
        print(f"ğŸ• å½“å‰åŒ—äº¬æ—¶é—´ï¼š{datetime.now(beijing_tz).strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ç«‹å³è¿è¡Œä¸€æ¬¡
        print("\nğŸš€ é¦–æ¬¡è¿è¡Œ...")
        job()
        
        # æŒç»­ç›‘å¬è°ƒåº¦
        print("\nâ³ ç­‰å¾…ä¸‹æ¬¡è°ƒåº¦...")
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿è¡Œç›¸åº”æ¨¡å¼"""
    parser = argparse.ArgumentParser(description='å¯Œé€”æ–°é—»çˆ¬è™« - Dockerç‰ˆæœ¬')
    parser.add_argument('--mode', choices=['auto', 'test', 'fulltest'], default='test',
                       help='è¿è¡Œæ¨¡å¼ï¼šauto(è‡ªåŠ¨å®šæ—¶) æˆ– test(æµ‹è¯•è¿è¡Œ) æˆ– fulltest(å…¨é‡æµ‹è¯•)')
    parser.add_argument('--output-dir', default=None,
                       help='è¾“å‡ºç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„outputæ–‡ä»¶å¤¹')
    parser.add_argument('--max-workers', type=int, default=None,
                       help='æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--max-concurrent', type=int, default=None,
                       help='æœ€å¤§å¹¶å‘è¯·æ±‚æ•°')
    parser.add_argument('--request-delay', type=float, default=None,
                       help='è¯·æ±‚å»¶è¿Ÿ(ç§’)')
    parser.add_argument('--use-proxy', choices=['true', 'false'], default=None,
                       help='æ˜¯å¦ä½¿ç”¨ä»£ç†æ± ')
    
    args = parser.parse_args()
    
    # ä»ç¯å¢ƒå˜é‡å’Œå‘½ä»¤è¡Œå‚æ•°è·å–é…ç½®
    def get_config(env_name, arg_value, default_value, converter=str):
        if arg_value is not None:
            return converter(arg_value)
        env_value = os.getenv(env_name)
        if env_value is not None:
            return converter(env_value)
        return default_value
    
    # å¹¶å‘å‚æ•°é…ç½®
    max_workers = get_config('MAX_WORKERS', args.max_workers, 30, int)
    max_concurrent = get_config('MAX_CONCURRENT', args.max_concurrent, 25, int)
    request_delay = get_config('REQUEST_DELAY', args.request_delay, 0.1, float)
    
    # ä»£ç†é…ç½®
    use_proxy_str = get_config('USE_PROXY', args.use_proxy, 'true', str)
    use_proxy = use_proxy_str.lower() in ['true', '1', 'yes']
    
    # è¾“å‡ºç›®å½•é…ç½®
    if args.output_dir:
        output_dir = args.output_dir
    else:
        output_dir = os.getenv('OUTPUT_DIR')
        if not output_dir:
            # æ ¹æ®è¿è¡Œç¯å¢ƒè‡ªåŠ¨é€‰æ‹©
            if os.path.exists('/.dockerenv'):  # Dockerç¯å¢ƒæ£€æµ‹
                output_dir = '/etc/FUTUNews/output'
            else:  # æœ¬åœ°å¼€å‘ç¯å¢ƒ
                output_dir = 'output'
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = FutuNewsScraper(
        max_workers=max_workers,
        max_concurrent=max_concurrent,
        request_delay=request_delay,
        output_dir=output_dir,
        use_proxy=use_proxy
    )
    
    print("=== å¯Œé€”æ–°é—»çˆ¬è™« Dockerç‰ˆ ===")
    print(f"ğŸ¯ è¿è¡Œæ¨¡å¼: {args.mode}")  
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {scraper.output_dir}")
    print(f"ğŸš€ å¹¶å‘é…ç½®: {max_workers}çº¿ç¨‹, {max_concurrent}å¹¶å‘, {request_delay}så»¶è¿Ÿ")
    print(f"ğŸŒ ä»£ç†é…ç½®: {'å¯ç”¨' if use_proxy else 'ç¦ç”¨'}")
    
    # è°ƒè¯•ç¯å¢ƒå˜é‡è¯»å–
    print(f"ğŸ” ç¯å¢ƒå˜é‡è°ƒè¯•:")
    print(f"  MAX_WORKERSç¯å¢ƒå˜é‡: {os.getenv('MAX_WORKERS', 'None')}")
    print(f"  MAX_CONCURRENTç¯å¢ƒå˜é‡: {os.getenv('MAX_CONCURRENT', 'None')}")
    print(f"  USE_PROXYç¯å¢ƒå˜é‡: {os.getenv('USE_PROXY', 'None')}")
    print(f"  å®é™…ä½¿ç”¨çš„use_proxyå€¼: {use_proxy} (æ¥æº: {use_proxy_str})")
    
    try:
        if args.mode == 'auto':
            # è‡ªåŠ¨æ¨¡å¼ï¼šå®šæ—¶è¿è¡Œ
            scraper.schedule_auto_mode()
        elif args.mode == 'test':
            # æµ‹è¯•æ¨¡å¼ï¼šä¸€æ¬¡æ€§è¿è¡Œ
            scraper.run_test_mode()
        elif args.mode == 'fulltest':
            # å…¨é‡æµ‹è¯•æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰æ¸¯ç¾è‚¡
            scraper.run_full_test_mode()
        else:
            print("âŒ æ— æ•ˆçš„è¿è¡Œæ¨¡å¼")
            return 1
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        return 0
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå¼‚å¸¸: {e}")
        return 1
    
    print("\nğŸ‰ ç¨‹åºè¿è¡Œå®Œæˆï¼")
    return 0


if __name__ == "__main__":
    exit(main())